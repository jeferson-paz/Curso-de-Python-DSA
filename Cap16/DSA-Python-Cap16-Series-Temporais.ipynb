{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b061b4ec",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "\n",
    "## <font color='blue'>Fundamentos de Linguagem Python Para Análise de Dados e Data Science</font>\n",
    "\n",
    "## <font color='blue'>Análise de Séries Temporais em Python</font>\n",
    "\n",
    "Problema de Negócio:\n",
    "\n",
    "Usando dados históricos das vendas ao longo de 2023 seria possível prever o total de vendas em Janeiro/2024?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da367ec2",
   "metadata": {},
   "source": [
    "![DSA](imagens/dsa_cap16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d022f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.11.7\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4db14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba483b3",
   "metadata": {},
   "source": [
    "## Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa85302",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Carrega o dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/jeferson-paz/Curso-de-Python-DSA/raw/main/Cap16/dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m df_dsa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pazj8\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\pazj8\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\pazj8\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\pazj8\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\pazj8\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Carrega o dataset\n",
    "url = 'https://github.com/jeferson-paz/Curso-de-Python-DSA/raw/main/Cap16/dataset.csv'\n",
    "df_dsa = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a814225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90447657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f087b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsa.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8f0c6",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor mínimo da coluna data\n",
    "df_dsa['Data'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3646e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor máximo da coluna data\n",
    "df_dsa['Data'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte a coluna de data no tipo datetime\n",
    "df_dsa['Data'] = pd.to_datetime(df_dsa['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dffe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter o DataFrame em uma série temporal com a data como índice\n",
    "serie_temporal = df_dsa.set_index('Data')['Total_Vendas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(serie_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fefc0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f426fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fornece a frequência da série temporal (diária, neste caso)\n",
    "serie_temporal = serie_temporal.asfreq('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ad23f",
   "metadata": {},
   "source": [
    "## Análise Exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o gráfico da série temporal (sem formatação)\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(serie_temporal)\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Vendas')\n",
    "plt.title('Série Temporal de Vendas')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o gráfico da série temporal (com formatação)\n",
    "\n",
    "# Criar o gráfico da série temporal com layout de contraste\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(serie_temporal, color = 'white', linewidth = 2)\n",
    "\n",
    "# Configurar cores e estilo do gráfico\n",
    "plt.gca().set_facecolor('#2e03a3')\n",
    "plt.grid(color = 'yellow', linestyle = '--', linewidth = 0.5)\n",
    "\n",
    "# Configurar rótulos dos eixos, título e legenda\n",
    "plt.xlabel('Data', color = 'black', fontsize = 14)\n",
    "plt.ylabel('Vendas', color ='black', fontsize = 14)\n",
    "plt.title('Série Temporal de Vendas', color = 'black', fontsize = 18)\n",
    "\n",
    "# Configurar as cores dos eixos e dos ticks (marcadores)\n",
    "plt.tick_params(axis = 'x', colors  ='black')\n",
    "plt.tick_params(axis = 'y', colors = 'black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990a9fa",
   "metadata": {},
   "source": [
    "## Suavização Exponencial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb0fe7",
   "metadata": {},
   "source": [
    "A suavização exponencial é uma técnica de análise e previsão de séries temporais que aplica médias ponderadas aos dados históricos, onde os pesos diminuem exponencialmente à medida que os dados ficam mais antigos. A suavização exponencial é útil para lidar com tendências e sazonalidades nos dados, e para reduzir o ruído.\n",
    "\n",
    "### Suposições da Suavização Exponencial\n",
    "\n",
    "https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html\n",
    "\n",
    "No Statsmodels, a classe SimpleExpSmoothing é uma implementação da técnica de suavização exponencial simples, que é uma versão mais básica da suavização exponencial que não lida explicitamente com tendências e sazonalidades.\n",
    "\n",
    "O modelo SimpleExpSmoothing do Statsmodels, também conhecido como suavização exponencial simples ou média móvel exponencialmente ponderada, é uma técnica de suavização de séries temporais que atribui pesos decrescentes exponencialmente aos pontos de dados passados. Ele é usado principalmente para suavizar séries temporais e fazer previsões de curto prazo. As principais suposições do modelo SimpleExpSmoothing são as seguintes:\n",
    "\n",
    "- A série temporal é composta por um componente de nível (média) e um componente de erro aleatório (ruído). Não há componentes de tendência ou sazonalidade na série.\n",
    "\n",
    "\n",
    "- O componente de nível é uma média ponderada dos valores passados, com pesos que diminuem exponencialmente à medida que os dados ficam mais distantes no passado.\n",
    "\n",
    "\n",
    "- O componente de erro aleatório é normalmente distribuído com média zero e variância constante. Além disso, os erros são independentes e identicamente distribuídos.\n",
    "\n",
    "\n",
    "- O parâmetro de suavização (alfa) é uma constante entre 0 e 1, que determina a taxa de decaimento dos pesos. Valores próximos a 1 dão maior peso aos dados mais recentes, enquanto valores próximos a 0 dão mais peso aos dados mais antigos.\n",
    "\n",
    "O modelo SimpleExpSmoothing é uma técnica de suavização bastante simples que tem suas limitações. Ele é mais adequado para séries temporais que não apresentam tendências ou sazonalidades claras e para fazer previsões de curto prazo. Para séries temporais com componentes de tendência e/ou sazonalidade, modelos mais avançados como o ExponentialSmoothing de Holt-Winters ou modelos SARIMA podem ser mais apropriados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23824fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o modelo\n",
    "modelo = SimpleExpSmoothing(serie_temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff08635",
   "metadata": {},
   "source": [
    "Esta linha acima cria uma instância da classe SimpleExpSmoothing, utilizando a coluna 'Vendas' da série serie_temporal como entrada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento (ajuste) do modelo\n",
    "modelo_ajustado = modelo.fit(smoothing_level = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cd539",
   "metadata": {},
   "source": [
    "Esta linha acima faz uma chamada ao método fit() para ajustar o modelo de suavização exponencial aos dados. O argumento smoothing_level=0.2 define o parâmetro de suavização (alfa) como 0.2. O parâmetro de suavização controla a rapidez com que os pesos decrescem ao longo do tempo; um valor maior atribui mais peso aos dados mais recentes, enquanto um valor menor atribui mais peso aos dados mais antigos. O valor de alfa deve estar entre 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os valores previstos pelo modelo\n",
    "suavizacao_exponencial = modelo_ajustado.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563edb1",
   "metadata": {},
   "source": [
    "Esta linha acima extrai os valores ajustados do modelo de suavização exponencial. Os valores ajustados são as estimativas da série temporal suavizada, que são calculadas aplicando os pesos exponenciais aos dados históricos. Esses valores ajustados podem ser usados para analisar a série temporal suavizada, identificar tendências e comparar com outras técnicas de suavização ou previsão.\n",
    "\n",
    "O resultado final é uma nova série temporal chamada suavizacao_exponencial, que representa a versão suavizada da série original de vendas, com menos ruído e flutuações de curto prazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7600402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(serie_temporal, label = 'Valores Reais')\n",
    "plt.plot(suavizacao_exponencial, label = 'Valores Previstos', linestyle = '--')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Vendas')\n",
    "plt.title('Modelo de Suavização Exponencial')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3e48f",
   "metadata": {},
   "source": [
    "## Deploy e Previsão com o Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões\n",
    "num_previsoes = 1\n",
    "previsoes = modelo_ajustado.forecast(steps = num_previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Previsão do Total de Vendas Para Janeiro/2024:', round(previsoes[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22cb7c",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
